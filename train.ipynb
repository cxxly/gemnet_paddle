{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger\n",
    "import os\n",
    "import logging\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "os.environ[\"AUTOGRAPH_VERBOSITY\"] = \"1\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s (%(levelname)s): %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "import string\n",
    "import ast\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from gemnet.model.gemnet import GemNet\n",
    "from gemnet.training.trainer import Trainer\n",
    "from gemnet.training.metrics import Metrics, BestMetrics\n",
    "from gemnet.training.data_container import DataContainer\n",
    "from gemnet.training.data_provider import DataProvider\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)\n",
    "    \n",
    "# For strings that yaml doesn't parse (e.g. None)\n",
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_spherical = config[\"num_spherical\"]\n",
    "num_radial = config[\"num_radial\"]\n",
    "num_blocks = config[\"num_blocks\"]\n",
    "emb_size_atom = config[\"emb_size_atom\"]\n",
    "emb_size_edge = config[\"emb_size_edge\"]\n",
    "emb_size_trip = config[\"emb_size_trip\"]\n",
    "emb_size_quad = config[\"emb_size_quad\"]\n",
    "emb_size_rbf = config[\"emb_size_rbf\"]\n",
    "emb_size_cbf = config[\"emb_size_cbf\"]\n",
    "emb_size_sbf = config[\"emb_size_sbf\"]\n",
    "num_before_skip = config[\"num_before_skip\"]\n",
    "num_after_skip = config[\"num_after_skip\"]\n",
    "num_concat = config[\"num_concat\"]\n",
    "num_atom = config[\"num_atom\"]\n",
    "emb_size_bil_quad = config[\"emb_size_bil_quad\"]\n",
    "emb_size_bil_trip = config[\"emb_size_bil_trip\"]\n",
    "triplets_only = config[\"triplets_only\"]\n",
    "forces_coupled = config[\"forces_coupled\"]\n",
    "direct_forces = config[\"direct_forces\"]\n",
    "mve = config[\"mve\"]\n",
    "cutoff = config[\"cutoff\"]\n",
    "int_cutoff = config[\"int_cutoff\"]\n",
    "envelope_exponent = config[\"envelope_exponent\"]\n",
    "extensive = config[\"extensive\"]\n",
    "output_init = config[\"output_init\"]\n",
    "scale_file = config[\"scale_file\"]\n",
    "data_seed = config[\"data_seed\"]\n",
    "dataset = config[\"dataset\"]\n",
    "val_dataset = config[\"val_dataset\"]\n",
    "num_train = config[\"num_train\"]\n",
    "num_val = config[\"num_val\"]\n",
    "logdir = config[\"logdir\"]\n",
    "loss = config[\"loss\"]\n",
    "tfseed = config[\"tfseed\"]\n",
    "num_steps = config[\"num_steps\"]\n",
    "rho_force = config[\"rho_force\"]\n",
    "ema_decay = config[\"ema_decay\"]\n",
    "weight_decay = config[\"weight_decay\"]\n",
    "grad_clip_max = config[\"grad_clip_max\"]\n",
    "agc = config[\"agc\"]\n",
    "decay_patience = config[\"decay_patience\"]\n",
    "decay_factor = config[\"decay_factor\"]\n",
    "decay_cooldown = config[\"decay_cooldown\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "evaluation_interval = config[\"evaluation_interval\"]\n",
    "patience = config[\"patience\"]\n",
    "save_interval = config[\"save_interval\"]\n",
    "learning_rate = config[\"learning_rate\"]\n",
    "warmup_steps = config[\"warmup_steps\"]\n",
    "decay_steps = config[\"decay_steps\"]\n",
    "decay_rate = config[\"decay_rate\"]\n",
    "staircase = config[\"staircase\"]\n",
    "restart = config[\"restart\"]\n",
    "comment = config[\"comment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths and create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 00:39:58 (INFO): Start training\n",
      "2024-06-14 00:39:58 (INFO): Available GPUs: 7\n",
      "2024-06-14 00:39:58 (INFO): CUDA Available: True\n",
      "2024-06-14 00:39:58 (INFO): Directory: logs/20240614_003958_iaYEMI_coll_v1.2_train.npz_GemNet\n",
      "2024-06-14 00:39:58 (INFO): Create directories\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(tfseed)\n",
    "\n",
    "logging.info(\"Start training\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "cuda_available = torch.cuda.is_available()\n",
    "logging.info(f\"Available GPUs: {num_gpus}\")\n",
    "logging.info(f\"CUDA Available: {cuda_available}\")\n",
    "if num_gpus == 0:\n",
    "    logging.warning(\"No GPUs were found. Training is run on CPU!\")\n",
    "if not cuda_available:\n",
    "    logging.warning(\"CUDA unavailable. Training is run on CPU!\")\n",
    "\n",
    "# Used for creating a \"unique\" id for a run (almost impossible to generate the same twice)\n",
    "def id_generator(\n",
    "    size=6, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits\n",
    "):\n",
    "    return \"\".join(random.SystemRandom().choice(chars) for _ in range(size))\n",
    "\n",
    "# A unique directory name is created for this run based on the input\n",
    "if (restart is None) or (restart == \"None\"):\n",
    "    directory = (\n",
    "        logdir\n",
    "        + \"/\"\n",
    "        + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        + \"_\"\n",
    "        + id_generator()\n",
    "        + \"_\"\n",
    "        + os.path.basename(dataset)\n",
    "        + \"_\"\n",
    "        + str(comment)\n",
    "    )\n",
    "else:\n",
    "    directory = restart\n",
    "\n",
    "logging.info(f\"Directory: {directory}\")\n",
    "logging.info(\"Create directories\")\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "best_dir = os.path.join(directory, \"best\")\n",
    "if not os.path.exists(best_dir):\n",
    "    os.makedirs(best_dir)\n",
    "log_dir = os.path.join(directory, \"logs\")\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "extension = \".pth\"\n",
    "log_path_model = f\"{log_dir}/model{extension}\"\n",
    "log_path_training = f\"{log_dir}/training{extension}\"\n",
    "best_path_model = f\"{best_dir}/model{extension}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 00:39:58 (INFO): Initialize model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GemNet(\n",
       "  (rbf_basis): BesselBasisLayer(\n",
       "    (envelope): Envelope()\n",
       "  )\n",
       "  (cbf_basis): SphericalBasisLayer(\n",
       "    (envelope): Envelope()\n",
       "  )\n",
       "  (sbf_basis): TensorBasisLayer(\n",
       "    (envelope): Envelope()\n",
       "  )\n",
       "  (cbf_basis3): SphericalBasisLayer(\n",
       "    (envelope): Envelope()\n",
       "  )\n",
       "  (mlp_rbf4): Dense(\n",
       "    (linear): Linear(in_features=6, out_features=16, bias=False)\n",
       "    (_activation): Identity()\n",
       "  )\n",
       "  (mlp_cbf4): Dense(\n",
       "    (linear): Linear(in_features=42, out_features=16, bias=False)\n",
       "    (_activation): Identity()\n",
       "  )\n",
       "  (mlp_sbf4): EfficientInteractionDownProjection()\n",
       "  (mlp_rbf3): Dense(\n",
       "    (linear): Linear(in_features=6, out_features=16, bias=False)\n",
       "    (_activation): Identity()\n",
       "  )\n",
       "  (mlp_cbf3): EfficientInteractionDownProjection()\n",
       "  (mlp_rbf_h): Dense(\n",
       "    (linear): Linear(in_features=6, out_features=16, bias=False)\n",
       "    (_activation): Identity()\n",
       "  )\n",
       "  (mlp_rbf_out): Dense(\n",
       "    (linear): Linear(in_features=6, out_features=16, bias=False)\n",
       "    (_activation): Identity()\n",
       "  )\n",
       "  (atom_emb): AtomEmbedding(\n",
       "    (embeddings): Embedding(93, 128)\n",
       "  )\n",
       "  (edge_emb): EdgeEmbedding(\n",
       "    (dense): Dense(\n",
       "      (linear): Linear(in_features=262, out_features=128, bias=False)\n",
       "      (_activation): ScaledSiLU(\n",
       "        (_activation): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out_blocks): ModuleList(\n",
       "    (0-4): 5 x OutputBlock(\n",
       "      (dense_rbf): Dense(\n",
       "        (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "        (_activation): Identity()\n",
       "      )\n",
       "      (scale_sum): ScalingFactor()\n",
       "      (layers): ModuleList(\n",
       "        (0): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (1-2): 2 x ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (seq_energy): ModuleList(\n",
       "        (0): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (1-2): 2 x ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (out_energy): Dense(\n",
       "        (linear): Linear(in_features=128, out_features=1, bias=False)\n",
       "        (_activation): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (int_blocks): ModuleList(\n",
       "    (0-3): 4 x InteractionBlock(\n",
       "      (dense_ca): Dense(\n",
       "        (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (_activation): ScaledSiLU(\n",
       "          (_activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (quad_interaction): QuadrupletInteraction(\n",
       "        (dense_db): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (mlp_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_rbf): ScalingFactor()\n",
       "        (mlp_cbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=32, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_cbf): ScalingFactor()\n",
       "        (mlp_sbf): EfficientInteractionBilinear()\n",
       "        (scale_sbf_sum): ScalingFactor()\n",
       "        (down_projection): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ca): Dense(\n",
       "          (linear): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ac): Dense(\n",
       "          (linear): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (trip_interaction): TripletInteraction(\n",
       "        (dense_ba): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (mlp_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_rbf): ScalingFactor()\n",
       "        (mlp_cbf): EfficientInteractionBilinear()\n",
       "        (scale_cbf_sum): ScalingFactor()\n",
       "        (down_projection): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=64, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ca): Dense(\n",
       "          (linear): Linear(in_features=64, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ac): Dense(\n",
       "          (linear): Linear(in_features=64, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers_before_skip): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers_after_skip): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (atom_update): AtomUpdateBlock(\n",
       "        (dense_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_sum): ScalingFactor()\n",
       "        (layers): ModuleList(\n",
       "          (0): Dense(\n",
       "            (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (_activation): ScaledSiLU(\n",
       "              (_activation): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (1-2): 2 x ResidualLayer(\n",
       "            (dense_mlp): Sequential(\n",
       "              (0): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "              (1): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (concat_layer): EdgeEmbedding(\n",
       "        (dense): Dense(\n",
       "          (linear): Linear(in_features=384, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (residual_m): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.info(\"Initialize model\")\n",
    "model = GemNet(\n",
    "    num_spherical=num_spherical,\n",
    "    num_radial=num_radial,\n",
    "    num_blocks=num_blocks,\n",
    "    emb_size_atom=emb_size_atom,\n",
    "    emb_size_edge=emb_size_edge,\n",
    "    emb_size_trip=emb_size_trip,\n",
    "    emb_size_quad=emb_size_quad,\n",
    "    emb_size_rbf=emb_size_rbf,\n",
    "    emb_size_cbf=emb_size_cbf,\n",
    "    emb_size_sbf=emb_size_sbf,\n",
    "    num_before_skip=num_before_skip,\n",
    "    num_after_skip=num_after_skip,\n",
    "    num_concat=num_concat,\n",
    "    num_atom=num_atom,\n",
    "    emb_size_bil_quad=emb_size_bil_quad,\n",
    "    emb_size_bil_trip=emb_size_bil_trip,\n",
    "    num_targets=2 if mve else 1,\n",
    "    triplets_only=triplets_only,\n",
    "    direct_forces=direct_forces,\n",
    "    forces_coupled=forces_coupled,\n",
    "    cutoff=cutoff,\n",
    "    int_cutoff=int_cutoff,\n",
    "    envelope_exponent=envelope_exponent,\n",
    "    activation=\"swish\",\n",
    "    extensive=extensive,\n",
    "    output_init=output_init,\n",
    "    scale_file=scale_file,\n",
    ")\n",
    "# push to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 00:41:40 (INFO): Load dataset\n",
      "2024-06-14 00:41:40 (INFO): Training data size: 120000\n",
      "2024-06-14 00:41:40 (INFO): Validation data size: 10000\n"
     ]
    }
   ],
   "source": [
    "train = {}\n",
    "validation = {}\n",
    "\n",
    "logging.info(\"Load dataset\")\n",
    "data_container = DataContainer(\n",
    "    dataset, cutoff=cutoff, int_cutoff=int_cutoff, triplets_only=triplets_only\n",
    ")\n",
    "\n",
    "if val_dataset is not None:\n",
    "    # Initialize DataProvider\n",
    "    if num_train == 0:\n",
    "        num_train = len(data_container)\n",
    "    logging.info(f\"Training data size: {num_train}\")\n",
    "    data_provider = DataProvider(\n",
    "        data_container,\n",
    "        num_train,\n",
    "        0,\n",
    "        batch_size,\n",
    "        seed=data_seed,\n",
    "        shuffle=True,\n",
    "        random_split=True,\n",
    "    )\n",
    "\n",
    "    # Initialize validation datasets\n",
    "    val_data_container = DataContainer(\n",
    "        val_dataset,\n",
    "        cutoff=cutoff,\n",
    "        int_cutoff=int_cutoff,\n",
    "        triplets_only=triplets_only,\n",
    "    )\n",
    "    if num_val == 0:\n",
    "        num_val = len(val_data_container)\n",
    "    logging.info(f\"Validation data size: {num_val}\")\n",
    "    val_data_provider = DataProvider(\n",
    "        val_data_container,\n",
    "        0,\n",
    "        num_val,\n",
    "        batch_size,\n",
    "        seed=data_seed,\n",
    "        shuffle=True,\n",
    "        random_split=True,\n",
    "    )\n",
    "else:\n",
    "    # Initialize DataProvider (splits dataset into 3 sets based on data_seed and provides tf.datasets)\n",
    "    logging.info(f\"Training data size: {num_train}\")\n",
    "    logging.info(f\"Validation data size: {num_val}\")\n",
    "    assert num_train > 0\n",
    "    assert num_val > 0\n",
    "    data_provider = DataProvider(\n",
    "        data_container,\n",
    "        num_train,\n",
    "        num_val,\n",
    "        batch_size,\n",
    "        seed=data_seed,\n",
    "        shuffle=True,\n",
    "        random_split=True,\n",
    "    )\n",
    "    val_data_provider = data_provider\n",
    "\n",
    "# Initialize datasets\n",
    "train[\"dataset_iter\"] = data_provider.get_dataset(\"train\")\n",
    "validation[\"dataset_iter\"] = val_data_provider.get_dataset(\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 00:41:40 (INFO): Prepare training\n",
      "2024-06-14 00:41:40 (INFO): Freshly initialize model\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Prepare training\")\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    learning_rate=learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    weight_decay=weight_decay,\n",
    "    ema_decay=ema_decay,\n",
    "    decay_patience=decay_patience,\n",
    "    decay_factor=decay_factor,\n",
    "    decay_cooldown=decay_cooldown,\n",
    "    grad_clip_max=grad_clip_max,\n",
    "    rho_force=rho_force,\n",
    "    mve=mve,\n",
    "    loss=loss,\n",
    "    staircase=staircase,\n",
    "    agc=agc,\n",
    ")\n",
    "\n",
    "# Initialize metrics\n",
    "train[\"metrics\"] = Metrics(\"train\", trainer.tracked_metrics)\n",
    "validation[\"metrics\"] = Metrics(\"val\", trainer.tracked_metrics)\n",
    "\n",
    "# Save/load best recorded loss (only the best model is saved)\n",
    "metrics_best = BestMetrics(best_dir, validation[\"metrics\"])\n",
    "\n",
    "# Set up checkpointing\n",
    "# Restore latest checkpoint\n",
    "if os.path.exists(log_path_model):\n",
    "    logging.info(\"Restoring model and trainer\")\n",
    "    model_checkpoint = torch.load(log_path_model)\n",
    "    model.load_state_dict(model_checkpoint[\"model\"])\n",
    "\n",
    "    train_checkpoint = torch.load(log_path_training)\n",
    "    trainer.load_state_dict(train_checkpoint[\"trainer\"])\n",
    "    # restore the best saved results\n",
    "    metrics_best.restore()\n",
    "    logging.info(f\"Restored best metrics: {metrics_best.loss}\")\n",
    "    step_init = int(train_checkpoint[\"step\"])\n",
    "else:\n",
    "    logging.info(\"Freshly initialize model\")\n",
    "    metrics_best.inititalize()\n",
    "    step_init = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/chenxiaoxu02/workspaces/gemnet_pytorch/train.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://icoding%2B325365.icoding.baidu-int.com/home/chenxiaoxu02/workspaces/gemnet_pytorch/train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     summary_writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m, lr, global_step\u001b[39m=\u001b[39mstep)\n\u001b[1;32m     <a href='vscode-notebook-cell://icoding%2B325365.icoding.baidu-int.com/home/chenxiaoxu02/workspaces/gemnet_pytorch/train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Perform training step\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://icoding%2B325365.icoding.baidu-int.com/home/chenxiaoxu02/workspaces/gemnet_pytorch/train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain_on_batch(train[\u001b[39m\"\u001b[39;49m\u001b[39mdataset_iter\u001b[39;49m\u001b[39m\"\u001b[39;49m], train[\u001b[39m\"\u001b[39;49m\u001b[39mmetrics\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://icoding%2B325365.icoding.baidu-int.com/home/chenxiaoxu02/workspaces/gemnet_pytorch/train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Save progress\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icoding%2B325365.icoding.baidu-int.com/home/chenxiaoxu02/workspaces/gemnet_pytorch/train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m save_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/home/chenxiaoxu02/workspaces/gemnet_pytorch/gemnet/training/trainer.py:327\u001b[0m, in \u001b[0;36mTrainer.train_on_batch\u001b[0;34m(self, dataset_iter, metrics)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_on_batch\u001b[39m(\u001b[39mself\u001b[39m, dataset_iter, metrics):\n\u001b[1;32m    326\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m--> 327\u001b[0m     inputs, targets \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataset_iter)\n\u001b[1;32m    328\u001b[0m     \u001b[39m# push to GPU if available\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     inputs, targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdict2device(inputs), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdict2device(targets)\n",
      "File \u001b[0;32m/home/chenxiaoxu02/workspaces/gemnet_pytorch/gemnet/training/data_provider.py:171\u001b[0m, in \u001b[0;36mDataProvider.get_dataset.<locals>.generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerator\u001b[39m():\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mfor\u001b[39;49;00m inputs, targets \u001b[39min\u001b[39;49;00m dataloader:\n\u001b[1;32m    172\u001b[0m             \u001b[39myield\u001b[39;49;00m inputs, targets\n",
      "File \u001b[0;32m/home/chenxiaoxu02/anaconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/home/chenxiaoxu02/anaconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/home/chenxiaoxu02/anaconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/home/chenxiaoxu02/anaconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/home/chenxiaoxu02/workspaces/gemnet_pytorch/gemnet/training/data_container.py:261\u001b[0m, in \u001b[0;36mDataContainer.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39m# get adjacency matrix for embeddings\u001b[39;00m\n\u001b[1;32m    260\u001b[0m adj_mat \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39mcsr_matrix(D_ij \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcutoff)\n\u001b[0;32m--> 261\u001b[0m adj_mat \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39meye(n, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mbool)\n\u001b[1;32m    262\u001b[0m adj_matrices\u001b[39m.\u001b[39mappend(adj_mat)\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtriplets_only:\n\u001b[1;32m    265\u001b[0m     \u001b[39m# get adjacency matrix for interaction\u001b[39;00m\n",
      "File \u001b[0;32m/home/chenxiaoxu02/anaconda3/envs/py311/lib/python3.11/site-packages/numpy/__init__.py:284\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mimport\u001b[39;00m Tester\n\u001b[1;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m Tester\n\u001b[0;32m--> 284\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'"
     ]
    }
   ],
   "source": [
    "summary_writer = SummaryWriter(log_dir)\n",
    "steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "\n",
    "for step in range(step_init + 1, num_steps + 1):\n",
    "\n",
    "    # keep track of the learning rate\n",
    "    if step % 10 == 0:\n",
    "        lr = trainer.schedulers[0].get_last_lr()[0]\n",
    "        summary_writer.add_scalar(\"lr\", lr, global_step=step)\n",
    "\n",
    "    # Perform training step\n",
    "    trainer.train_on_batch(train[\"dataset_iter\"], train[\"metrics\"])\n",
    "\n",
    "    # Save progress\n",
    "    if step % save_interval == 0:\n",
    "        torch.save({\"model\": model.state_dict()}, log_path_model)\n",
    "        torch.save(\n",
    "            {\"trainer\": trainer.state_dict(), \"step\": step}, log_path_training\n",
    "        )\n",
    "\n",
    "    # Check performance on the validation set\n",
    "    if step % evaluation_interval == 0:\n",
    "\n",
    "        # Save backup variables and load averaged variables\n",
    "        trainer.save_variable_backups()\n",
    "        trainer.load_averaged_variables()\n",
    "\n",
    "        # Compute averages\n",
    "        for i in range(int(np.ceil(num_val / batch_size))):\n",
    "            trainer.test_on_batch(validation[\"dataset_iter\"], validation[\"metrics\"])\n",
    "\n",
    "        # Update and save best result\n",
    "        if validation[\"metrics\"].loss < metrics_best.loss:\n",
    "            metrics_best.update(step, validation[\"metrics\"])\n",
    "            torch.save(model.state_dict(), best_path_model)\n",
    "\n",
    "        # write to summary writer\n",
    "        metrics_best.write(summary_writer, step)\n",
    "\n",
    "        epoch = step // steps_per_epoch\n",
    "        train_metrics_res = train[\"metrics\"].result(append_tag=False)\n",
    "        val_metrics_res = validation[\"metrics\"].result(append_tag=False)\n",
    "        metrics_strings = [\n",
    "            f\"{key}: train={train_metrics_res[key]:.6f}, val={val_metrics_res[key]:.6f}\"\n",
    "            for key in validation[\"metrics\"].keys\n",
    "        ]\n",
    "        logging.info(\n",
    "            f\"{step}/{num_steps} (epoch {epoch}): \" + \"; \".join(metrics_strings)\n",
    "        )\n",
    "\n",
    "        # decay learning rate on plateau\n",
    "        trainer.decay_maybe(validation[\"metrics\"].loss)\n",
    "\n",
    "        train[\"metrics\"].write(summary_writer, step)\n",
    "        validation[\"metrics\"].write(summary_writer, step)\n",
    "        train[\"metrics\"].reset_states()\n",
    "        validation[\"metrics\"].reset_states()\n",
    "\n",
    "        # Restore backup variables\n",
    "        trainer.restore_variable_backups()\n",
    "\n",
    "        # early stopping\n",
    "        if step - metrics_best.step > patience * evaluation_interval:\n",
    "            break\n",
    "\n",
    "result = {key + \"_best\": val for key, val in metrics_best.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in metrics_best.items():\n",
    "    print(f\"{key}: {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d9d58ddb04bb635eba824a3c64b6d0110bcc4c6cff8b192a6f7cbbb2bf10de4"
  },
  "kernelspec": {
   "display_name": "Python 3.5.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
